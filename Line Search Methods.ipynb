{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # arrays\n",
    "import matplotlib.pyplot as plt  # 2d plotting\n",
    "from mpl_toolkits.mplot3d import axes3d  # 3d plotting\n",
    "from numpy.linalg import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QObjFunc(x): # f\n",
    "    return (x[0] - 1)**2 + (2*x[1] - 1)**2\n",
    "def QGradObjFunc(x): # gradient of f\n",
    "    return np.array([2*(x[0] - 1), 2*(2*x[1] - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_step_steepest_decent_method(objFunc, gradObjFunc, alpha, x0, tol, maxIter):\n",
    "  path = [x0]\n",
    "  k = 0\n",
    "  xk = x0\n",
    "  pk = -gradObjFunc(x0)\n",
    "  while norm(pk) > tol and k <= maxIter:\n",
    "    xk = xk + alpha * pk # alpha is fixed: alpha = 0.01\n",
    "    pk = -gradObjFunc(xk)\n",
    "    k = k + 1\n",
    "    path.append(xk)\n",
    "\n",
    "  path = np.array(path) # convert to array\n",
    "\n",
    "  if norm(pk) <= tol:\n",
    "     print(\"Found the minimizer at {x} with {iter} iterations successfully, \\\n",
    "gradient's norm is {nrm}.\".format(x=xk,iter=k,nrm=norm(pk)))\n",
    "  else:\n",
    "     print(\"Unable to locate minimizer within maximum iterations, last \\\n",
    "position is at {x}, gradient's norm is {nrm}\".format(x=xk,nrm=norm(pk)))\n",
    "\n",
    "  return xk, k, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the minimizer at [1.  0.5] with 23 iterations successfully, gradient's norm is 4.222124871944294e-09.\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.3 #I chose this value because it is approximately in the middle of the range of alpha\n",
    "tol = 1e-8\n",
    "x0 = np.array([4, 1])\n",
    "maxIter = 1e6\n",
    "\n",
    "x_2, iter_2, path_2 = fixed_step_steepest_decent_method(QObjFunc, QGradObjFunc, alpha, x0, tol, maxIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999439031875"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_2,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]   # limit value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the minimizer at [1.  0.5] with 40 iterations successfully, gradient's norm is 8.020496711225178e-09.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6000000199329446"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_2 = 0.2 \n",
    "x_3, iter_3, path_3 = fixed_step_steepest_decent_method(QObjFunc, QGradObjFunc, alpha_2, x0, tol, maxIter)\n",
    "err = np.linalg.norm(np.diff(path_3,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]   # limit value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WolfeI(alpha,f,x,y,dx,p,c1=0.1):\n",
    "    '''Return True/False if Wolfe condition I is satisfied for the given alpha'''\n",
    "    LHS = f(x+alpha*p[0], y+alpha*p[1])\n",
    "    RHS = f(x,y)-c1*alpha*np.dot(dx,p)\n",
    "    return LHS <= RHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 1,100    # parameters for Rosenbrock function\n",
    "f = lambda x,y: (a-x)**2+b*(y-x**2)**2\n",
    "Df = lambda x,y: np.array([2*(x-a)-4*b*x*(y-x**2),\n",
    "                           2*b*(y-x**2)])\n",
    "D2f = lambda x,y: np.array([[2-4*b*y+12*b*x**2,-4*b*x],\n",
    "                            [-4*b*x,2*b]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration 1, alpha=1 after 0 backtracks, and newx=1.1959183673469387\n",
      "In iteration 2, alpha=0.5625 after 2 backtracks, and newx=1.0860802602485584\n",
      "In iteration 3, alpha=1 after 0 backtracks, and newx=1.0608688670326778\n",
      "In iteration 4, alpha=1 after 0 backtracks, and newx=1.0068651122729955\n",
      "In iteration 5, alpha=1 after 0 backtracks, and newx=1.002529108862995\n",
      "In iteration 6, alpha=1 after 0 backtracks, and newx=1.000009474292406\n",
      "In iteration 7, alpha=1 after 0 backtracks, and newx=1.0000000120143648\n",
      "In iteration 8, alpha=1 after 0 backtracks, and newx=1.0000000000000002\n",
      "After 8 iterations, approximate minimum is 4.930380657631324e-32 at (1.0000000000000002, 1.0000000000000004)\n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2    # initial point\n",
    "path_Newton = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "rho = 0.75            # rho for backtracking\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -np.linalg.solve(D2f(x,y),dx)  # faster to solve a system than manually invert\n",
    "    alpha = 1\n",
    "    j = 0   # keep track of how many backtracking iterations\n",
    "    while not WolfeI(alpha,f,x,y,dx,pk):\n",
    "        alpha *= rho\n",
    "        j += 1\n",
    "        if i == 0:\n",
    "            print(f'In iteration {i}, 1st backtrack: alpha={alpha}')\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_Newton.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "    print(f'In iteration {i}, alpha={alpha} after {j} backtracks, and newx={x}')\n",
    "\n",
    "path_Newton=np.array(path_Newton)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 SR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_convergence_rate(path, minimizer, numToAvg=100, showPlot=False):\n",
    "    '''Given a path defined by an iteration and a known minimizer, approximates convergence rate'''\n",
    "    err = np.linalg.norm(path-np.array(minimizer),axis=1) # ||x_k-x*||=e_k\n",
    "    \n",
    "    # if converged in very few steps, return infinite order\n",
    "    if len(err)<=3:\n",
    "        return np.inf\n",
    "    \n",
    "    pp = np.zeros(len(err)-3)\n",
    "    for i in range(len(err)-3):\n",
    "        pp[i] = np.log(err[i+2]/err[i+1])/np.log(err[i+1]/err[i])\n",
    "    \n",
    "    if numToAvg>len(pp):\n",
    "        # if not enough iterations to average, just average all\n",
    "        p=np.mean(pp)\n",
    "    else:\n",
    "        # return mean of last few iterations\n",
    "        p=np.mean(pp[-numToAvg:])\n",
    "        \n",
    "    # plot\n",
    "    if showPlot:\n",
    "        plt.plot(pp)\n",
    "        plt.plot(pp*0+p)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('p')\n",
    "        plt.title(f'p={p}')\n",
    "        plt.show()\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar #Line search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objFunc(x): # Rosenbrock \n",
    "    return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "def GradObjFunc(x): # gradient of Rosenbrock\n",
    "    return np.array([400*x[0]**3-400*x[0]*x[1]+2*x[0]-2, -200*x[0]**2+200*x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 iterations, approximate minimum is 1.9721522630525295e-31 at (1.0000000000000004, 1.0000000000000009)\n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2    # initial point\n",
    "path_SR1 = [[x,y]]\n",
    "# same tol and max steps\n",
    "i=0                   # iteration count\n",
    "skips = 0             # keep track of how many times SR1 update is skipped\n",
    "H = np.eye(2)         # initial inverse Hessian is identity\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -H@dx\n",
    "    \n",
    "    def subproblem1D(alpha):\n",
    "      return objFunc([x,y] + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "    alpha = res.x\n",
    "                \n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_SR1.append([xnew,ynew])\n",
    "    \n",
    "    # secant variables\n",
    "    sk = alpha*pk         # x_{k+1}-x_k\n",
    "    yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k\n",
    "    \n",
    "    # SR1 update\n",
    "    vec = sk-H@yk     # @ is mATrix multiplication\n",
    "    denom = vec@yk    # computes dot product if two vectors\n",
    "#     print('Denominator is ',denom)   # uncomment this to do a little inspection\n",
    "    if abs(denom)>1e-8*np.linalg.norm(vec)*np.linalg.norm(yk):\n",
    "        # only update if denominator doesn't vanish\n",
    "        H += np.outer(vec,vec)/denom    # outer product of two vectors works better than a@b.T, which requires reshaping\n",
    "    else:\n",
    "        # if denominator vanishes, alert the user\n",
    "        skips += 1\n",
    "        if skips<50:\n",
    "            # set limit so we don't print too much\n",
    "            print('Skipping SR1 update in iteration',i,'. Denominator is ',denom)\n",
    "        elif skips==50:\n",
    "            print('Reached maximum number of skips; breaking loop now.')\n",
    "            break\n",
    "\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "\n",
    "path_SR1=np.array(path_SR1)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate convergence rate: 2.022308814658158\n"
     ]
    }
   ],
   "source": [
    "print('Approximate convergence rate:', approx_convergence_rate(path_SR1,[1,1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023157185918107678"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_SR1,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superlinear convergence rate because the limit value is close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 22 iterations, approximate minimum is 2.0023532838814214e-20 at (1.0000000001403417, 1.0000000002824938)\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1    # initial point\n",
    "path_SR1 = [[x,y]]\n",
    "# same tol and max steps\n",
    "i=0                   # iteration count\n",
    "skips = 0             # keep track of how many times SR1 update is skipped\n",
    "H = np.eye(2)         # initial inverse Hessian is identity\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -H@dx\n",
    "    \n",
    "    def subproblem1D(alpha):\n",
    "      return objFunc([x,y] + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "    alpha = res.x\n",
    "                \n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_SR1.append([xnew,ynew])\n",
    "    \n",
    "    # secant variables\n",
    "    sk = alpha*pk         # x_{k+1}-x_k\n",
    "    yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k\n",
    "    \n",
    "    # SR1 update\n",
    "    vec = sk-H@yk     # @ is mATrix multiplication\n",
    "    denom = vec@yk    # computes dot product if two vectors\n",
    "#     print('Denominator is ',denom)   # uncomment this to do a little inspection\n",
    "    if abs(denom)>1e-8*np.linalg.norm(vec)*np.linalg.norm(yk):\n",
    "        # only update if denominator doesn't vanish\n",
    "        H += np.outer(vec,vec)/denom    # outer product of two vectors works better than a@b.T, which requires reshaping\n",
    "    else:\n",
    "        # if denominator vanishes, alert the user\n",
    "        skips += 1\n",
    "        if skips<50:\n",
    "            # set limit so we don't print too much\n",
    "            print('Skipping SR1 update in iteration',i,'. Denominator is ',denom)\n",
    "        elif skips==50:\n",
    "            print('Reached maximum number of skips; breaking loop now.')\n",
    "            break\n",
    "\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "\n",
    "path_SR1=np.array(path_SR1)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate convergence rate: -8.227601851867034\n"
     ]
    }
   ],
   "source": [
    "print('Approximate convergence rate:', approx_convergence_rate(path_SR1,[1,1],1,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010971334318083837"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_SR1,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superlinear convergence rate because the limit value is close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFP Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 iterations, approximate minimum is 1.9721522630525295e-31 at (1.0000000000000004, 1.0000000000000009)\n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2\n",
    "path_DFP = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "H = np.eye(2)         # initial inverse Hessian is identity\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -H@dx   # -H * gradient\n",
    "    \n",
    "    def subproblem1D(alpha):\n",
    "      return objFunc([x,y] + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "    alpha = res.x\n",
    "        \n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_DFP.append([xnew,ynew])\n",
    "    \n",
    "    # secant variables\n",
    "    sk = alpha*pk         # x_{k+1}-x_k   x_{k+1} = x_k + alpha * pk\n",
    "    yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k  \n",
    "    \n",
    "    # DFP update\n",
    "    vec = H@yk\n",
    "    denom1 = yk@sk\n",
    "    denom2 = yk@vec\n",
    "    H += np.outer(sk,sk)/denom1 - np.outer(vec,vec)/denom2 # np.outer(vec, vec) = vec * vec^T\n",
    "\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "\n",
    "path_DFP=np.array(path_DFP)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate convergence rate: 9.997557599671778\n"
     ]
    }
   ],
   "source": [
    "print('Approximate convergence rate:', approx_convergence_rate(path_DFP,[1,1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023280839890673632"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_DFP,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superlinear convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 22 iterations, approximate minimum is 2.0010566028157128e-20 at (1.0000000001402927, 1.0000000002823979)\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1\n",
    "path_DFP = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "H = np.eye(2)         # initial inverse Hessian is identity\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -H@dx   # -H * gradient\n",
    "    \n",
    "    def subproblem1D(alpha):\n",
    "      return objFunc([x,y] + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "    alpha = res.x\n",
    "        \n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_DFP.append([xnew,ynew])\n",
    "    \n",
    "    # secant variables\n",
    "    sk = alpha*pk         # x_{k+1}-x_k   x_{k+1} = x_k + alpha * pk\n",
    "    yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k  \n",
    "    \n",
    "    # DFP update\n",
    "    vec = H@yk\n",
    "    denom1 = yk@sk\n",
    "    denom2 = yk@vec\n",
    "    H += np.outer(sk,sk)/denom1 - np.outer(vec,vec)/denom2 # np.outer(vec, vec) = vec * vec^T\n",
    "\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "\n",
    "path_DFP=np.array(path_DFP)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate convergence rate: -4.20782340802865\n"
     ]
    }
   ],
   "source": [
    "print('Approximate convergence rate:', approx_convergence_rate(path_DFP,[1,1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010966746026993221"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_DFP,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "superlinear convergence rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10 iterations, approximate minimum is 1.2449211160519093e-30 at (0.9999999999999999, 0.9999999999999999)\n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2    # initial point\n",
    "path_BFGS = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "H = np.eye(2)         # initial inverse Hessian is identity\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -H@dx\n",
    "    \n",
    "    def subproblem1D(alpha):\n",
    "      return objFunc([x,y] + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "    alpha = res.x\n",
    "        \n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_BFGS.append([xnew,ynew])\n",
    "    \n",
    "    # secant variables\n",
    "    sk = alpha*pk         # x_{k+1}-x_k\n",
    "    yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k\n",
    "    \n",
    "    # BFGS update\n",
    "    vec = H@yk\n",
    "    denom = yk@sk\n",
    "    H += (denom+vec@yk)*np.outer(sk,sk)/denom**2 - (np.outer(vec,sk)+np.outer(sk,vec))/denom\n",
    "\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "\n",
    "path_BFGS=np.array(path_BFGS)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate convergence rate: 2.0212010565576857\n"
     ]
    }
   ],
   "source": [
    "print('Approximate convergence rate:', approx_convergence_rate(path_BFGS,[1,1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002326323557902368"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_BFGS,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superlinear convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 22 iterations, approximate minimum is 2.002112254270269e-20 at (1.0000000001403309, 1.0000000002824738)\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1    # initial point\n",
    "path_BFGS = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "H = np.eye(2)         # initial inverse Hessian is identity\n",
    "dx = Df(x,y)          # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -H@dx\n",
    "    \n",
    "    def subproblem1D(alpha):\n",
    "      return objFunc([x,y] + alpha * pk)\n",
    "    res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "    alpha = res.x\n",
    "        \n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    path_BFGS.append([xnew,ynew])\n",
    "    \n",
    "    # secant variables\n",
    "    sk = alpha*pk         # x_{k+1}-x_k\n",
    "    yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k\n",
    "    \n",
    "    # BFGS update\n",
    "    vec = H@yk\n",
    "    denom = yk@sk\n",
    "    H += (denom+vec@yk)*np.outer(sk,sk)/denom**2 - (np.outer(vec,sk)+np.outer(sk,vec))/denom\n",
    "\n",
    "    x,y = xnew,ynew\n",
    "    dx = Df(x,y)\n",
    "    i += 1\n",
    "\n",
    "path_BFGS=np.array(path_BFGS)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate convergence rate: -8.227744871469328\n"
     ]
    }
   ],
   "source": [
    "print('Approximate convergence rate:', approx_convergence_rate(path_BFGS,[1,1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010967919824387476"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.linalg.norm(np.diff(path_BFGS,axis=0),axis=1) # ||x_{k+1}-x_k||\n",
    "err[-1]/err[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superlinear convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
